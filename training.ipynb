{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from torch.nn.functional import threshold, normalize\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from  tqdm import tqdm\n",
    "from statistics import mean\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0,0,0,0), lw=2)) \n",
    "\n",
    "def show_mask(mask, ax, random_color=False):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask_image = mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    ax.imshow(mask_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "device = \"cpu\"\n",
    "sam_model = sam_model_registry[\"vit_h\"](checkpoint=\"sam_vit_h_4b8939.pth\")\n",
    "sam_model.to(device=device)\n",
    "sam_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()\n",
    "lr = 1e-4\n",
    "wd = 0\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=lr, weight_decay=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ../ImageSets load the image_ids from the train.txt file\n",
    "with open(\"/Users/ioanna/Downloads/FoodSeg103/ImageSets/train.txt\", \"r\") as f:\n",
    "    image_ids = f.read().splitlines()\n",
    "    # remove .jpg from the image_ids\n",
    "    image_ids = [image_id[:-4].strip() for image_id in image_ids]\n",
    "\n",
    "# shuffle the image_ids\n",
    "np.random.shuffle(image_ids)\n",
    "# subset the image_ids to 1000\n",
    "image_ids = image_ids[:1000]\n",
    "# split the image_ids into train and val sets 90% train, 10% val\n",
    "train_ids = image_ids[:int(len(image_ids)*0.9)]\n",
    "val_ids = image_ids[int(len(image_ids)*0.9):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "losses = []\n",
    "val_losses = []\n",
    "mean_val_losses = []\n",
    "patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    sam_model.train()\n",
    "    epoch_losses = []\n",
    "    epoch_val_losses = []\n",
    "\n",
    "    for image_id in tqdm(train_ids):\n",
    "        # load mask \n",
    "        mask_path = \"/Users/ioanna/Downloads/FoodSeg103/Images/ann_dir/train/{}.png\".format(image_id)\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "        # load image\n",
    "        image = cv2.imread(\"/Users/ioanna/Downloads/FoodSeg103/Images/img_dir/train/{}.jpg\".format(image_id))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        mask_temp = mask.copy()\n",
    "\n",
    "\n",
    "        for unique_value in np.unique(mask):\n",
    "            if unique_value == 0:\n",
    "                continue\n",
    "            # set all other values to 0\n",
    "            mask_temp[mask != unique_value] = 0\n",
    "            # set all other values to 1\n",
    "            mask_temp[mask == unique_value] = 1\n",
    "\n",
    "            bbox = np.argwhere(mask_temp)\n",
    "            \n",
    "            # get bounding box coordinates\n",
    "            (y1, x1), (y2, x2) = bbox.min(0), bbox.max(0) + 1\n",
    "            # to nnumpy\n",
    "            bbox_coords = np.array([x1, y1, x2, y2])\n",
    "\n",
    "             \n",
    "            # plt.figure(figsize=(10,10))\n",
    "            # plt.imshow(image)\n",
    "            # show_box(bbox, plt.gca())\n",
    "            # show_mask(mask_temp, plt.gca())\n",
    "            # plt.axis('off')\n",
    "            # plt.show()\n",
    "\n",
    "            transform = ResizeLongestSide(target_length= sam_model.image_encoder.img_size)\n",
    "            input_image = transform.apply_image(image)\n",
    "            input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "            transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "            input_image = sam_model.preprocess(transformed_image)\n",
    "\n",
    "            original_image_size = image.shape[:2]\n",
    "            input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "            with torch.no_grad():\n",
    "                image_embedding = sam_model.image_encoder(input_image)\n",
    "            \n",
    "            \n",
    "            box = transform.apply_boxes(bbox_coords, original_image_size)\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "            box_torch = box_torch[None, :]\n",
    "            \n",
    "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box_torch,\n",
    "                masks=None,\n",
    "            )\n",
    "            low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding,\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "            sparse_prompt_embeddings=sparse_embeddings,\n",
    "            dense_prompt_embeddings=dense_embeddings,\n",
    "            multimask_output=False,\n",
    "            )\n",
    "\n",
    "            upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "            binary_mask = normalize(threshold(upscaled_masks, 0.0, 0)).to(device)\n",
    "\n",
    "            gt_mask_resized = torch.from_numpy(np.resize(mask_temp, (1, 1, mask_temp.shape[0], mask_temp.shape[1]))).to(device)\n",
    "            gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "            \n",
    "            loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_losses.append(loss.item())\n",
    "            print(f'Loss: {loss.item()}')\n",
    "            \n",
    "    losses.append(mean(epoch_losses))\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {mean(epoch_losses)}')\n",
    "    # save checkpoint of model\n",
    "    torch.save(sam_model.state_dict(), 'trained_foodseg103_checkpoint.pth')    \n",
    "\n",
    "    sam_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # val set\n",
    "        for val_image_id in tqdm(val_ids):\n",
    "\n",
    "            # load mask \n",
    "            mask_path = \"/Users/ioanna/Downloads/FoodSeg103/Images/ann_dir/train/{}.png\".format(val_image_id)\n",
    "            mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
    "            # load image\n",
    "            image = cv2.imread(\"/Users/ioanna/Downloads/FoodSeg103/Images/img_dir/train/{}.jpg\".format(val_image_id))\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            mask_temp = mask.copy()\n",
    "\n",
    "            for unique_value in np.unique(mask):\n",
    "                if unique_value == 0:\n",
    "                    continue\n",
    "                # set all other values to 0\n",
    "                mask_temp[mask != unique_value] = 0\n",
    "                # set all other values to 1\n",
    "                mask_temp[mask == unique_value] = 1\n",
    "\n",
    "                bbox = np.argwhere(mask_temp)\n",
    "                \n",
    "                # get bounding box coordinates\n",
    "                (y1, x1), (y2, x2) = bbox.min(0), bbox.max(0) + 1\n",
    "                # to nnumpy\n",
    "                bbox_coords = np.array([x1, y1, x2, y2])\n",
    "\n",
    "                \n",
    "                # plt.figure(figsize=(10,10))\n",
    "                # plt.imshow(image)\n",
    "                # show_box(bbox, plt.gca())\n",
    "                # show_mask(mask_temp, plt.gca())\n",
    "                # plt.axis('off')\n",
    "                # plt.show()\n",
    "\n",
    "                transform = ResizeLongestSide(target_length= sam_model.image_encoder.img_size)\n",
    "                input_image = transform.apply_image(image)\n",
    "                input_image_torch = torch.as_tensor(input_image, device=device)\n",
    "                transformed_image = input_image_torch.permute(2, 0, 1).contiguous()[None, :, :, :]\n",
    "                input_image = sam_model.preprocess(transformed_image)\n",
    "\n",
    "                original_image_size = image.shape[:2]\n",
    "                input_size = tuple(transformed_image.shape[-2:])\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    image_embedding = sam_model.image_encoder(input_image)\n",
    "                \n",
    "                \n",
    "                box = transform.apply_boxes(bbox_coords, original_image_size)\n",
    "                box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "                box_torch = box_torch[None, :]\n",
    "                \n",
    "                sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                    points=None,\n",
    "                    boxes=box_torch,\n",
    "                    masks=None,\n",
    "                )\n",
    "                low_res_masks, iou_predictions = sam_model.mask_decoder(\n",
    "                image_embeddings=image_embedding,\n",
    "                image_pe=sam_model.prompt_encoder.get_dense_pe(),\n",
    "                sparse_prompt_embeddings=sparse_embeddings,\n",
    "                dense_prompt_embeddings=dense_embeddings,\n",
    "                multimask_output=False,\n",
    "                )\n",
    "\n",
    "                upscaled_masks = sam_model.postprocess_masks(low_res_masks, input_size, original_image_size).to(device)\n",
    "                binary_mask = normalize(threshold(upscaled_masks, 0.0, 0)).to(device)\n",
    "                gt_mask_resized = torch.from_numpy(np.resize(mask_temp, (1, 1, mask_temp.shape[0], mask_temp.shape[1]))).to(device)\n",
    "                gt_binary_mask = torch.as_tensor(gt_mask_resized > 0, dtype=torch.float32)\n",
    "                val_loss = loss_fn(binary_mask, gt_binary_mask)\n",
    "                epoch_val_losses.append(val_loss.item())\n",
    "\n",
    "\n",
    "\n",
    "    val_losses.append(mean(epoch_val_losses))\n",
    "    \n",
    "    # if validation error is increasing, stop training\n",
    "    if mean(epoch_val_losses) > min(val_losses):\n",
    "        print('Validation loss is increasing')\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter == patience:\n",
    "            print('Early stopping')\n",
    "            break\n",
    "    else:\n",
    "        early_stopping_counter = 0\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# save epoch losses\n",
    "with open('losses103.txt', 'w') as f:\n",
    "    for item in losses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# save validation losses\n",
    "with open('val_losses103.txt', 'w') as f:\n",
    "    for item in val_losses:\n",
    "        f.write(\"%s\\n\" % item)\n",
    "\n",
    "# save model\n",
    "torch.save(sam_model.state_dict(), 'trained_foodseg103.pth')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
